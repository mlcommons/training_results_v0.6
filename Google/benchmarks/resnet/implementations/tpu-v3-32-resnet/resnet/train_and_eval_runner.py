# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Bypass TPUEstimator for ResNet-50 Train."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import functools
import math
import os
import threading
import time
from absl import flags
from six.moves import queue as Queue
import tensorflow as tf

from tensorflow.contrib import tpu
from tensorflow.contrib.tpu.python.tpu import tpu_function
from tensorflow.core.protobuf import rewriter_config_pb2
from tensorflow.python.data.util import nest as data_nest
from tensorflow.python.framework import graph_io
from mlp_log import mlp_log

FLAGS = flags.FLAGS

_INITIAL_LOSS = 1e7
_STOP = -1


# Decorator function for tpu computation func that was passed to tpu.rewrite()
# if there are embedded train and eval loops in this func, trace tools will
# generate step markers for each iteration.
def on_device_train_and_eval_loops(func):
  # Value for this attribute is from xla.DebugOptions.StepMarkerLocation.
  setattr(func, "step_marker_location", "STEP_MARK_AT_SECOND_LEVEL_WHILE_LOOP")
  return func


def device_for_tpu_core(host_name, core=0):
  return host_name + "/device:TPU_REPLICATED_CORE:%d" % core


def device_for_host(host_name):
  return host_name + "/device:CPU:0"


def wrap_computation_in_while_loop(op_fn, n, host_name, parallel_iterations=1):
  """Wraps the ops generated by `op_fn` in tf.while_loop."""

  def computation(i):
    ops = op_fn()
    if not isinstance(ops, list):
      ops = [ops]
    with tf.control_dependencies(ops):
      return i + 1

  with tf.device(device_for_host(host_name)):
    return tf.while_loop(
        lambda i: tf.less(i, n),
        computation, [tf.constant(0)],
        parallel_iterations=parallel_iterations)


def tpu_ordinal_fn(shard_index_in_host):
  """Return the TPU ordinal associated with a shard.

  Required because the enqueue ops are placed on CPU.

  Args:
    shard_index_in_host: the shard index

  Returns:
    The ordinal of the TPU device the shard's infeed should be placed on.
  """
  return shard_index_in_host % FLAGS.tpu_cores_per_host


def _profiler_callback(comment, session_id):
  if session_id is None:
    tf.logging.info("Profiling failed for %s", comment)
  else:
    tf.logging.info("Profiling succeeded for %s. Overview page url:", comment)


class TrainAndEvalRunner(object):
  """Remove init overheads in TPU Estimator via direct session.run calls."""

  def __init__(self, iterations, train_steps, eval_steps):
    tf.logging.info("TrainAndEvalRunner: constructor")
    self.feature_structure = {}
    self.eval_feature_structure = {}
    self.loss = None
    self.eval_loss = None
    self.infeed_queue = []
    self.eval_infeed_queue = []
    self.enqueue_ops = []
    self.num_hosts = FLAGS.num_cores // FLAGS.tpu_cores_per_host
    self.dequeue_ops = []
    self.queue = Queue.Queue()
    self.eval_enqueue_ops = []
    self.dataset_initializer = []
    self.eval_dataset_initializer = []
    self.iterations = iterations
    self.steps_per_epoch = FLAGS.num_train_images // FLAGS.train_batch_size
    self.iterator = None
    self.sess = None
    self.input_sess = None
    self.eval_input_sess = None
    self.eval_output_sess = None
    self.infeed_thread = None
    self.train_eval_thread = None
    self.graph = tf.Graph()
    self.input_graph = tf.Graph()
    self.eval_input_graph = tf.Graph()
    self.eval_output_graph = tf.Graph()
    if train_steps % iterations != 0:
      train_steps = iterations * int(math.ceil(train_steps / iterations))
    self.train_steps = train_steps
    self.max_train_iterations = self.train_steps // iterations
    self.eval_steps = int(eval_steps)
    self.eval_batch_size = FLAGS.eval_batch_size
    tpu_init = [tpu.initialize_system()]
    self.tpu_shutdown = tpu.shutdown_system()
    self.tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
        FLAGS.tpu or FLAGS.master,
        zone=FLAGS.tpu_zone,
        project=FLAGS.gcp_project)
    self.config = tf.ConfigProto(
        operation_timeout_in_ms=600 * 60 * 1000,
        allow_soft_placement=True,
        graph_options=tf.GraphOptions(
            rewrite_options=rewriter_config_pb2.RewriterConfig(
                disable_meta_optimizer=True)),
        isolate_session_state=True)
    cluster_spec = self.tpu_cluster_resolver.cluster_spec()
    if cluster_spec:
      self.config.cluster_def.CopyFrom(cluster_spec.as_cluster_def())
    self.master = self.tpu_cluster_resolver.get_master()
    self.init_sess = tf.Session(self.master, config=self.config)
    self.init_sess.run(tpu_init)

  def get_host(self, host_id):
    if self.master in ("", "local"):
      return "/replica:0/task:0"
    job_name = self.tpu_cluster_resolver.get_job_name() or "tpu_worker"
    return "/job:%s/task:%d" % (job_name, host_id)

  def build_enqueue_ops(self, input_fn, params, host_id, is_training=True):
    """Build enqueue operations for the input pipeline in a given host.

    Args:
      input_fn: dataset input graph generation function
      params:  input function parameters
      host_id:  host identifier
      is_training: boolean indicates if it is training
    """

    iparams = {}
    iparams["batch_size"] = params["batch_size"] // FLAGS.num_cores
    iparams["dataset_num_shards"] = self.num_hosts

    def get_enqueue_ops_fn():
      """Generate the enqueue ops graph function."""

      iparams["dataset_index"] = host_id
      with tf.device(device_for_host(self.get_host(host_id))):
        dataset = input_fn(iparams)
        if not is_training:
          dataset = dataset.cache()
          dataset = dataset.repeat()
        iterator = dataset.make_initializable_iterator()
        if is_training:
          self.dataset_initializer.append(iterator.initializer)
        else:
          self.eval_dataset_initializer.append(iterator.initializer)

        def enqueue_ops_fn():
          """Generate the infeed enqueue ops graph."""

          per_host_sharded_inputs = []
          control_deps = []
          for _ in range(FLAGS.tpu_cores_per_host):
            with tf.control_dependencies(control_deps):
              features, labels = iterator.get_next()
            if is_training:
              self.feature_structure["features"] = features
              self.feature_structure["labels"] = labels
              flattened_inputs = data_nest.flatten(self.feature_structure)
            else:
              self.eval_feature_structure["features"] = features
              self.eval_feature_structure["labels"] = labels
              flattened_inputs = data_nest.flatten(self.eval_feature_structure)

            control_deps.extend(flattened_inputs)
            per_host_sharded_inputs.append(flattened_inputs)

          infeed = tpu.InfeedQueue(
              number_of_tuple_elements=len(per_host_sharded_inputs[0]))
          if is_training:
            self.infeed_queue.append(infeed)
          else:
            self.eval_infeed_queue.append(infeed)
          return infeed.generate_enqueue_ops(
              per_host_sharded_inputs, tpu_ordinal_function=tpu_ordinal_fn)

      return enqueue_ops_fn

    if is_training:
      with self.input_graph.as_default():
        self.enqueue_ops.append(
            wrap_computation_in_while_loop(
                get_enqueue_ops_fn(),
                n=self.iterations,
                host_name=self.get_host(host_id),
                parallel_iterations=1))
    else:
      with self.eval_input_graph.as_default():
        self.eval_enqueue_ops.append(
            wrap_computation_in_while_loop(
                get_enqueue_ops_fn(),
                host_name=self.get_host(host_id),
                n=self.eval_steps,
                parallel_iterations=1))

  def get_tpu_step(self, mparams, model_fn, is_training=True):
    """Get the TPU graph generation function."""

    def tpu_step(loss):
      """Generate the TPU graph."""
      del loss
      if is_training:
        values = self.infeed_queue[0].generate_dequeue_op(tpu_device=0)
        unflattened_inputs = data_nest.pack_sequence_as(self.feature_structure,
                                                        values)
      else:
        values = self.eval_infeed_queue[0].generate_dequeue_op(tpu_device=0)
        unflattened_inputs = data_nest.pack_sequence_as(
            self.eval_feature_structure, values)

      features = unflattened_inputs["features"]
      labels = unflattened_inputs["labels"]
      if is_training:
        estimator_spec = model_fn(features, labels, tf.estimator.ModeKeys.TRAIN,
                                  mparams)
        loss, train_op = estimator_spec.loss, estimator_spec.train_op
        with tf.device(device_for_tpu_core(self.get_host(0))):
          with tf.control_dependencies([train_op]):
            return tf.identity(loss)
      else:
        estimator_spec = model_fn(features, labels, tf.estimator.ModeKeys.EVAL,
                                  mparams)
        loss = estimator_spec.loss
        self.eval_metrics = estimator_spec.eval_metrics
        self.eval_tensors = estimator_spec.eval_metrics[1]
        for _ in self.eval_tensors:
          self.dequeue_ops.append([])
        with tf.device(device_for_tpu_core(self.get_host(0))):
          outfeed_enqueue_ops = tpu.outfeed_enqueue_tuple(self.eval_tensors)
          with tf.control_dependencies([outfeed_enqueue_ops]):
            return tf.identity(loss)

    return tpu_step

  def launch_profiler(self):
    """Launches a profiling session to collect a trace from worker-0."""
    if result == profiler_client.PROFILED_IN_NEW_THREAD:
      tf.logging.info("A profiler session launched in a new thread.")
    else:
      tf.logging.info("profiler.collect() failed.")

  def initialize(self, train_input_fn, eval_input_fn, model_fn, params):
    """Build graphs for the TPU device and the input pipelines.

    Args:
      train_input_fn: Dataset input graph generation function for training.
      eval_input_fn: Dataset input graph generation function for training.
      model_fn: Model definition function
      params:  Parameters to input and model functions
    """

    tf.logging.info("TrainAndEvalRunner: initialize method")

    self.build_enqueue_ops(train_input_fn, params, 0)

    # Start the build of the model
    tpu_step = self.get_tpu_step(params, model_fn)

    @tpu_function.on_device_training_loop
    def train_loop():
      with tf.variable_scope("resnet", reuse=tf.AUTO_REUSE):
        return tpu.repeat(self.iterations, tpu_step, [_INITIAL_LOSS])

    self.train_loop = train_loop

    # Build tpu train model session and initialize graph
    self.initialize_eval(params, eval_input_fn, model_fn)

    # Build the infeed graph
    i = 1
    while i < self.num_hosts:
      self.build_enqueue_ops(train_input_fn, params, i)
      i = i + 1

    self.sess = tf.Session(self.master, graph=self.graph, config=self.config)

    self.input_sess = tf.Session(
        self.master, graph=self.input_graph, config=self.config)

    self.input_sess.run(self.dataset_initializer)

    self.eval_input_sess = tf.Session(
        self.master, graph=self.eval_input_graph, config=self.config)

    self.eval_input_sess.run(self.eval_dataset_initializer)

    self.eval_output_sess = tf.Session(
        self.master, graph=self.eval_output_graph, config=self.config)

    with self.graph.as_default():
      self.sess.run(tf.global_variables_initializer())
      self.sess.run(tf.local_variables_initializer())

    def train_eval_thread_fn(sess, train_eval_op):
      sess.run([train_eval_op])

    # Start the just in time compilation of the model function
    self.train_eval_thread = threading.Thread(
        target=train_eval_thread_fn, args=(self.sess, self.train_eval_op))
    self.train_eval_thread.start()

    # Sleep for JTC to finish
    time.sleep(60)

  def initialize_eval(self, params, eval_input_fn, model_fn):
    """Initialize eval."""

    self.eval_infeed_queue = []

    for i in range(0, self.num_hosts):
      self.build_enqueue_ops(
          eval_input_fn, params, host_id=i, is_training=False)

    eval_step = self.get_tpu_step(params, model_fn, is_training=False)

    @tpu_function.on_device_training_loop
    def eval_loop():
      with tf.variable_scope("resnet", reuse=tf.AUTO_REUSE):
        return tpu.repeat(int(self.eval_steps), eval_step, [_INITIAL_LOSS])

    def train_eval_step(loss):
      del loss
      with tf.control_dependencies(self.train_loop()):
        return eval_loop()

    @on_device_train_and_eval_loops
    def train_eval_loop():
      return tpu.repeat(self.max_train_iterations, train_eval_step,
                        [_INITIAL_LOSS])

    def create_dequeue_ops(host_id):
      """Create deque ops graph function."""
      dequeue_ops = []
      tensor_dtypes = []
      tensor_shapes = []
      for v in self.eval_tensors:
        dequeue_ops.append([])
        tensor_dtypes.append(v.dtype)
        tensor_shapes.append(v.shape)
      for i in range(FLAGS.tpu_cores_per_host):
        with tf.device(device_for_host(self.get_host(host_id))):
          outfeed_tensors = tpu.outfeed_dequeue_tuple(
              dtypes=tensor_dtypes, shapes=tensor_shapes, device_ordinal=i)
          for j, item in enumerate(outfeed_tensors):
            dequeue_ops[j].append(item)
      for j in range(len(outfeed_tensors)):
        dequeue_ops[j] = tf.concat(dequeue_ops[j], axis=0)
      return dequeue_ops

    with self.graph.as_default():
      with tf.variable_scope("resnet", reuse=True):
        (self.train_eval_op,) = tpu.shard(
            train_eval_loop,
            inputs=[],
            num_shards=FLAGS.num_cores,
            outputs_from_all_shards=False)

        graph_io.write_graph(tf.Graph().as_graph_def(add_shapes=True),
                             FLAGS.model_dir, "graph.pbtxt")

    with self.eval_output_graph.as_default():
      with tf.variable_scope("resnet", reuse=True):
        for i in range(0, self.num_hosts):
          host_dequeue_ops = create_dequeue_ops(i)
          for j, dequeue_tenor in enumerate(host_dequeue_ops):
            self.dequeue_ops[j].append(dequeue_tenor)

        for j, _ in enumerate(self.eval_tensors):
          self.dequeue_ops[j] = tf.concat(self.dequeue_ops[j], axis=0)

        with tf.device(device_for_host(self.get_host(0))):
          metrics = self.eval_metrics[0](*self.dequeue_ops)
        metric_update_ops = []
        metric_value_ops = {}
        for (k, v) in metrics.items():
          metric_update_ops.append(v[1])
          metric_value_ops[k] = v[0]
        self.metric_update_ops = metric_update_ops
        self.metric_value_ops = metric_value_ops

        self.metric_initializer = tf.variables_initializer(
            tf.get_collection(tf.GraphKeys.METRIC_VARIABLES))

  def train_and_eval(self, output_summaries=False, enable_tracing=True):
    """Run the Train steps on the TPU device."""
    if output_summaries:
      output_dir = os.path.join(FLAGS.model_dir, "eval")
      tf.gfile.MakeDirs(output_dir)
      # Summary writer writes out eval metrics.
      summary_writer = tf.summary.FileWriter(output_dir)

    def infeed_thread_fn():
      """Build and infeed session.run calls in a background thread."""
      # Build infeed sesssion
      # Run infeed session.run calls
      tf.logging.info("Start infeed thread")
      for _ in range(self.train_steps // self.iterations):
        self.input_sess.run([self.enqueue_ops])
        self.eval_input_sess.run([self.eval_enqueue_ops])

    self.infeed_thread = threading.Thread(target=infeed_thread_fn)
    self.infeed_thread.start()

    # Gather trace for the first few steps.
    if enable_tracing:
      self.launch_profiler()

    cur_step = 0
    success = False
    while cur_step < self.train_steps:
      start = time.time()
      tf.logging.info("TrainAndEvalRunner: start next %d steps",
                      self.iterations)
      cur_step += self.iterations
      epoch = cur_step // self.steps_per_epoch - 1
      mlp_log.mlperf_print(
          "block_start", None, metadata={"first_epoch_num": epoch + 1,
                                         "epoch_count": 4})
      eval_results = self.eval(self.eval_steps)
      end = time.time()
      tf.logging.info(
          "TrainAndEvalRunner: step {} step time {} sec {} examples/sec".format(
              cur_step, end - start,
              self.iterations * FLAGS.train_batch_size / (end - start)))
      # Run eval.
      # Write out summary to tensorboard.
      if output_summaries:
        with tf.Graph().as_default():
          summaries = []
          for metric in eval_results:
            summaries.append(
                tf.Summary.Value(tag=metric, simple_value=eval_results[metric]))
            tf_summary = tf.Summary(value=list(summaries))
            summary_writer.add_summary(tf_summary, cur_step)
      # MLPerf logging for eval results.
      mlp_log.mlperf_print(
          "eval_accuracy",
          float(eval_results["top_1_accuracy"]),
          metadata={"epoch_num": epoch + 1})

      mlp_log.mlperf_print(
          "block_stop", None, metadata={"first_epoch_num": epoch + 1})
      tf.logging.info("Eval results at step %d: %s", cur_step, eval_results)
      if eval_results["top_1_accuracy"] >= FLAGS.stop_threshold:
        success = True
        mlp_log.mlperf_print("run_stop", None, metadata={"status": "success"})
        break

      if enable_tracing and cur_step > self.train_steps // 4:
        self.launch_profiler()
        enable_tracing = False

    if not success:
      mlp_log.mlperf_print("run_stop", None, metadata={"status": "abort"})

    mlp_log.mlperf_print("run_final", None)
    if output_summaries:
      summary_writer.close()

  def eval(self, num_steps):
    """Run the Eval steps on the TPU device.

    Args:
      num_steps: number of steps to run eval

    Returns:
      A dictionary of evaluation results.
    """

    self.eval_output_sess.run(self.metric_initializer)

    eval_results = {}
    tf.logging.info("Starting Eval on %d steps batch size %d" %
                    (num_steps, self.eval_batch_size))

    for _ in range(num_steps):
      _ = self.eval_output_sess.run(self.metric_update_ops)
    # Compute eval metrics
    session_out = self.eval_output_sess.run(self.metric_value_ops)
    eval_results["top_1_accuracy"] = session_out["top_1_accuracy"]

    return eval_results

  def shutdown(self):
    self.queue.put(_STOP)
    self.train_eval_thread.join()
    self.infeed_thread.join()
    self.sess.close()
