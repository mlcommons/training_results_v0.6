# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Bypass TPUEstimator for ResNet-50 Eval."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import threading
import time

from absl import flags
import absl.logging as _logging  # pylint: disable=unused-import
import tensorflow as tf

from tensorflow.contrib import tpu
from tensorflow.contrib.tpu.python.tpu import tpu_function
from tensorflow.python.data.util import nest as data_nest

FLAGS = flags.FLAGS

_INITIAL_LOSS = 1e7


def wrap_computation_in_while_loop(op_fn, n, parallel_iterations=10):
  """Wraps the ops generated by `op_fn` in tf.while_loop."""

  def computation(i):
    ops = op_fn()
    if not isinstance(ops, list):
      ops = [ops]
    with tf.control_dependencies(ops):
      return i + 1

  return tf.while_loop(
      lambda i: tf.less(i, n),
      computation, [tf.constant(0)],
      parallel_iterations=parallel_iterations)

def device_for_host(task=0, cpu=0):
  job_name = FLAGS.tpu_job_name or "tpu_worker"
  return "/job:%s/task:%d/device:CPU:%d" % (job_name, task, cpu)


def device_for_tpu_core(task=0, core=0):
  job_name = FLAGS.tpu_job_name or "tpu_worker"
  return "/job:%s/task:%d/device:TPU_REPLICATED_CORE:%d" % (job_name, task,
                                                            core)


def tpu_ordinal_fn(shard_index_in_host):
  """Return the TPU ordinal associated with a shard.

    Required because the enqueue ops are placed on CPU.

    Args:
        shard_index_in_host: the shard index

    Returns:
        The ordinal of the TPU device the shard's infeed should be placed on.
    """
  return shard_index_in_host % FLAGS.num_cores


class EvalRunner(object):
  """Run Eval via direct session.run calls eliminating init and compilation

   overheads from TPU Estimator.
  """

  def __init__(self, input_fn, model_fn, params, num_steps):
    self.feature_structure = {}
    self.loss = None
    self.enqueue_ops = None
    self.metric_initializer = None
    self.iterator = None
    self.batch_size = params["batch_size"]
    with tf.Graph().as_default() as self.graph:
      self.build_model(params, input_fn, model_fn, num_steps)
      self.tpu_init = tpu.initialize_system()
      initializer = tf.global_variables_initializer()
      self.tpu_shutdown = tpu.shutdown_system()
      self.local_initializer = tf.local_variables_initializer()
      self.saver = tf.train.Saver()

    cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
        FLAGS.tpu or FLAGS.master)
    self.sess = tf.Session(cluster_resolver.get_master(), graph=self.graph)
    self.sess.run(self.tpu_init)
    self.sess.run(initializer)
    self.sess.run(self.local_initializer)
    self.sess.run(self.iterator.initializer)

  def build_model(self, params, input_fn, model_fn, num_steps):
    """Build the TPU model and infeed enqueue ops."""

    iparams = {}
    iparams["batch_size"] = params["batch_size"] // FLAGS.num_cores

    def get_tpu_step(mparams):
      """Get the TPU graph generation function."""

      def tpu_step(loss, *args):
        """Generate the TPU graph."""
        del loss
        unflattened_inputs = data_nest.pack_sequence_as(self.feature_structure,
                                                        args)
        features = unflattened_inputs["features"]
        labels = unflattened_inputs["labels"]
        estimator_spec = model_fn(features, labels, tf.estimator.ModeKeys.EVAL,
                                  mparams)
        loss = estimator_spec.loss
        self.eval_metrics = estimator_spec.eval_metrics
        self.eval_tensors = estimator_spec.eval_metrics[1]
        with tf.device(device_for_tpu_core()):
          outfeed_enqueue_ops = tpu.outfeed_enqueue_tuple(self.eval_tensors)
          with tf.control_dependencies([outfeed_enqueue_ops]):
            return tf.identity(loss)

      return tpu_step

    infeed_queue = []

    def get_enqueue_ops_fn():
      """Generate the enqueue ops graph function."""

      def enqueue_ops_fn():
        """Generate the infeed enqueue ops graph."""

        per_host_sharded_inputs = []
        control_deps = []
        with tf.device(device_for_host()):
          for _ in range(FLAGS.num_cores):
            with tf.control_dependencies(control_deps):
              features, labels = self.iterator.get_next()
              self.feature_structure["features"] = features
              self.feature_structure["labels"] = labels
              flattened_inputs = data_nest.flatten(self.feature_structure)
              control_deps.extend(flattened_inputs)
              per_host_sharded_inputs.append(flattened_inputs)

          infeed = tpu.InfeedQueue(
              number_of_tuple_elements=len(per_host_sharded_inputs[0]))
          infeed_queue.append(infeed)
          return infeed.generate_enqueue_ops(per_host_sharded_inputs,
                                             tpu_ordinal_function=tpu_ordinal_fn)

      return enqueue_ops_fn

    with tf.device(device_for_host()):
      dataset = input_fn(iparams)
      dataset = dataset.cache()  # Cache the fully-generated eval dataset.
      dataset = dataset.repeat()  # Repeat indefinitely for unknown # of evals.
      self.iterator = dataset.make_initializable_iterator()
      self.enqueue_ops = wrap_computation_in_while_loop(
          get_enqueue_ops_fn(), n=num_steps, parallel_iterations=1)

    tpu_step = get_tpu_step(params)

    @tpu_function.on_device_training_loop
    def tpu_loop():
      return tpu.repeat(
          num_steps, tpu_step, [_INITIAL_LOSS], infeed_queue=infeed_queue[0])

    def create_dequeue_ops():
      dequeue_ops = []
      tensor_dtypes = []
      tensor_shapes = []
      for v in self.eval_tensors:
        dequeue_ops.append([])
        tensor_dtypes.append(v.dtype)
        tensor_shapes.append(v.shape)
        tf.logging.info("appending %s" % v.name)
      for i in range(FLAGS.num_cores):
        with tf.device(device_for_host()):
          outfeed_tensors = tpu.outfeed_dequeue_tuple(
              dtypes=tensor_dtypes,
              shapes=tensor_shapes,
              device_ordinal=i)
          for j, item in enumerate(outfeed_tensors):
            dequeue_ops[j].append(item)
      for j in range(len(outfeed_tensors)):
        dequeue_ops[j] = tf.concat(dequeue_ops[j], axis=0)
      return dequeue_ops

    (self.loss,) = tpu.shard(
        tpu_loop,
        inputs=[],
        num_shards=FLAGS.num_cores,
        outputs_from_all_shards=False)

    self.dequeue_ops = create_dequeue_ops()
    with tf.device(device_for_host()):
      metrics = self.eval_metrics[0](*self.dequeue_ops)
    metric_update_ops = []
    metric_value_ops = {}
    for (k, v) in metrics.items():
      # print("k: ", k)
      # print("v: ", v)
      metric_update_ops.append(v[1])
      metric_value_ops[k] = v[0]
    self.metric_update_ops = metric_update_ops
    self.metric_value_ops = metric_value_ops

    self.metric_initializer = tf.variables_initializer(
        tf.get_collection(tf.GraphKeys.METRIC_VARIABLES))


  def eval(self, num_steps, checkpoint_path):
    """Run the Eval steps on the TPU device.

    Args:
      num_steps: number of steps to run eval
      checkpoint_path: path to the checkpoint directory

    Returns:
      A dictionary of evaluation results.
    """

    start = time.time()
    self.sess.run(self.metric_initializer)
    self.saver.restore(self.sess, checkpoint_path)

    eval_results = {}
    def outfeed_thread_fn():
      tf.logging.info("start dequeue ops")
      for i in range(num_steps):
        _ = self.sess.run(self.metric_update_ops)
      # Compute eval metrics
      session_out = self.sess.run(self.metric_value_ops)
      eval_results["top_1_accuracy"] = session_out["top_1_accuracy"]

    def infeed_thread_fn(sess, enqueue_ops):
      sess.run([enqueue_ops])

    infeed_thread = threading.Thread(
        target=infeed_thread_fn, args=(self.sess, self.enqueue_ops))
    infeed_thread.start()
    outfeed_thread = threading.Thread(target=outfeed_thread_fn)
    outfeed_thread.start()
    tf.logging.info("Starting Eval on %d steps batch size %d" %
                    (num_steps, self.batch_size))
    loss = self.sess.run([self.loss])
    tf.logging.info("Eval Loss = {}".format(loss))
    infeed_thread.join()
    outfeed_thread.join()

    end = time.time()
    tf.logging.info("Eval performance: step time {} sec {} examples/sec".format(
        end - start, self.batch_size / (end - start)))

    return eval_results
